\subsection{Team Classification}
In sports video analytics, the classification of players into their respective teams plays a critical role. The identification of teams enables downstream tasks such as tactical analysis, ball possession inference, and offside detection. Traditionally, classification has relied heavily on handcrafted visual features, especially colour-histograms in \textit{HSV} or \textit{RGB} space. Then, using the colour histograms, classification is done by clustering techniques such as K-means Clustering or Mean-Shift Clustering~\cite{bradski2000opencv}. These pipelines are computationally efficient, however, they can fail in real-world settings where the kit colours can be similar, the lighting can vary significantly, or even occlusions can occur.

\subsubsection{Limitations of Colour-Based Classification}

The \textit{HSV (Hue, Saturation, and Value)} colour space has been widely adopted for colour-based classifications due to its separation of the chromatic and intensity information. This in turn makes it less sensitive to lighting changes~\cite{smith1978color}. However, such representations still remain shallow and lack the robustness against camera artifacts, compression, or partial occlusion, issues commonly faced in broadcast footage. Furthermore, it is often seen that the player in the crop boxes, are in different circumstances and positions and therefore the percentage occupied and the overall \textit{HSV} of the player can vary significantly.

\subsubsection{Vision-Language Embeddings with SigLIP}

To overcome these limitations, embeddings of the player detections are often used. The embeddings can be generated by \textit{Google DeepMind's} pre-trained model, \textit{SigLIP (Sigmoid Loss for Language Image Pre-Training)} ~\cite{zhai2023sigmoidlosslanguageimage}. The model is a vision-language model that builds on the popular model \textit{CLIP}. It does this by replacing the \textit{Softmax Contrastive Loss} with a \textit{Sigmoid-Based Loss}. This change results in the generation of better-calibrated and more expressive embeddings, particularly in the visual encoder. It also shows a more efficient performance, a key factor when real-life detection and tracking is needed. Although SigLIP is trained on image-text pairs, this task leverages only the pre-trained visual encoder to generate dense embeddings of the detected player crops. These embeddings capture appearance details beyond just the kit colour. The embedding also takes factors into account such as shape, texture, and shading which are critical under variable real-world conditions.


% \subsubsection{Embedding Extraction Pipeline}

% Player image crops are first extracted using bounding boxes from the object detector. These crops are then processed by the \texttt{SiglipVisionModel} and its corresponding \texttt{AutoProcessor} to produce high-dimensional embeddings. Each embedding is obtained by averaging the last hidden state of the model:

% \[
% \mathbf{e}_i = \frac{1}{L} \sum_{l=1}^{L} \mathbf{h}_i^{(l)}, \quad \mathbf{h}_i^{(l)} \in \mathbb{R}^D
% \]

% where \(L\) is the sequence length, \(D\) is the embedding dimension, and \(\mathbf{e}_i\) represents the final embedding for player \(i\).

\subsubsection{Embedding Extraction Pipeline}

Each detected player is cropped using bounding boxes from the object detector. Utilizing these detections, images are cropped from the frame, here teremed as \textit{crop boxes}. These crops are then passed through the \texttt{SiglipVisionModel} and processed by the corresponding \texttt{AutoProcessor} to yield dense vector embeddings. The embedding \(\mathbf{e}_i\) for a player crop is computed as the mean of the final hidden layer representations:

\[
\mathbf{e}_i = \frac{1}{L} \sum_{l=1}^{L} \mathbf{h}_i^{(l)}, \quad \mathbf{h}_i^{(l)} \in \mathbb{R}^D
\]

where \(L\) is the token sequence length and \(D\) is the embedding dimension.

\subsubsection{Dimensionality Reduction}

To prepare for clustering, embeddings are reduced in dimensionality using \textit{Uniform Manifold Approximation and Projection (UMAP)}~\cite{mcinnes2018umap}, a learning technique that converts data of high dimensions to lower ones without the loss of data. Here it converts the $768$ dimensional vectors to vectors of $3$ dimensions.

\subsubsection{Clustering}
The reduced embeddings are then clustered using the K-means Clustering technique with \(k=2\).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/kmeans.png}
    \caption{K-Means Clustering~\cite{kmeansImg}.}
    \label{fig:kmeans}
\end{figure}

 K-Means is an \textit{unsupervised} learning algorithm used to group data into $k$ distinct clusters as seen in Figure~\ref{fig:kmeans}. Each cluster is represented by its centroid, which is the mean of the data points in that cluster. The algorithm aims to minimize the total within-cluster variance, which is the sum of square of differences of between the data points and its centroid:

\[
\arg\min_{C} \sum_{i=1}^{k} \sum_{\mathbf{x} \in C_i} \| \mathbf{x} - \boldsymbol{\mu}_i \|^2
\]
where \(C_i\) is the \(i\)th cluster and \(\boldsymbol{\mu}_i\) its centroid.

The algorithm is explained as follows:

Let the input be a set of $n$ data points in a $D$-dimensional space, and let $k$ be the desired number of clusters.

\begin{enumerate}[label=\textbf{Step \arabic*.}, wide=0pt, leftmargin=*]

    \item \textbf{Initialize Centroids} \\
    Choose $k$ initial random centroids. As there are two teams, here it is taken as $2$.

    \item \textbf{Assign Data Points to Clusters} \\
    For each data point, compute the Euclidean distance to each centroid. Assign each point to the cluster whose centroid is closest.

    \item \textbf{Update Centroids} \\
    For each cluster, compute the new centroid by taking the mean of all data points assigned to that cluster.

    \item \textbf{Repeat Until Convergence} \\
    Repeat Steps 2 and 3 until the centroids no longer change significantly, or until a maximum number of iterations is reached. This indicates that the algorithm has converged.

\end{enumerate}




\subsubsection{Advantages Over colour-Based Methods}

Compared to the regular colour histogram-based approaches~\cite{smith1978color, zhang2006color}, this modern embedding-based method provides:

\begin{itemize}
    \item \textbf{Robustness to Visual Conditions:} Embeddings are less sensitive to changes in lighting, kit-similarity, occlusions, and different poses.
    \item \textbf{Modular Scalability:} The pipeline can be extended to supervised classification or semi-supervised propagation with minimal changes.
\end{itemize}

This approach offers a substantial advancement in player identity resolution, enhancing the reliability and scalability of football analytics systems.

